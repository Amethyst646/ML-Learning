# -*- coding: utf-8 -*-
"""Demented vs Non Demented Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PS1jAPW5-Yw90rnwjpTpp6-04iToQ1MI

**Demented vs. Nondemented Classification**
---



**Project Overview**

This project involves developing a Machine Learning model to classify patients as demented or non-demented. It is conducted independently using data from the [MRI and Alzheimer's Kaggle Dataset](https://www.kaggle.com/datasets/jboysen/mri-and-alzheimers?select=oasis_longitudinal.csv).

  **Objectives** :  The primary goal for this project is to understand each step in the machine learning development processes, creating a structured guide for developing and deploying the demented vs non-demented classification model. Key steps covered in this project are as follows:
  - `Data Collection`: Gathering and organizing relevant data for model development and training
  - `Data Preprocessing`: Cleaning and preparation stage before proceeding to the analysis stage.
  - `Exploratory Data Analysis (EDA)`: Analyzing the prepared data through visualization and identifying correlations or patterns within variables.
  - `Feature Engineering`: Feature selection to improve efficiency and model accuracy.
  - `Model Selection`: Testing and choosing the most suitable model for classification.
  - `Model Training`: Training the selected model to learn patterns.
  - `Model Evaluation`: Evaluating and giving assesment on model accuracy and effectiveness using several evaluation metrics.
  - `Model Deployment`: Deploying the model to make prediction in a production setting.

# **A. Importing Libraries**

<justify>

In any model development, not only in machine learning, the initial step is to declare all of the necessary libraries. Throughout this machine learning model development, we will be focusing heavily on several notable libraries. Their functions are as follows:
- `Pandas` : data analyzing, data cleaning, exploratory data, and data manipulation
- `Matplotlib` : data visualization and statistical plotting
- `Seaborn` : data visualization and statistical plotting
- `Scikit-learn` : model creation, feature engineering, evaluation, machine learning algorithm
- `Phik` : categorical feature selection

Although their occurences are not as frequent as the libraries above, several other libraries such as `Random`, `Pickle`, and `XGboost` are also implemented in this model development.
"""

# Install feature-engine library
pip install feature-engine

# Install phik library
pip install phik

# Import libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import random
import phik
import pickle

# Import libraries for Data Preprocessing
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.model_selection import train_test_split, KFold, cross_val_score
from feature_engine.outliers import Winsorizer
from sklearn.feature_selection import RFE

# Import libraries for Model Training & Evaluation
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import recall_score, precision_score, accuracy_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay

"""# **B. Data Collection**

The dataset acquired from Kaggle is stored in CSV format. To access the data, we can use the CSV reading function `pd.read_csv` provided by pandas.
"""

pdDS = pd.read_csv('/content/DementedvsNonDemented.csv')

"""Generally, as we manipulated data for analysis and then for model creation, there are possibilities of accidentally creating irreversible changes into the DataFrame we used. In that scenario, there are no other way except of starting over. An action which will take effort and time.

To mitigate this possibility, we will utilize pandas copy function `.copy()`. This function created a duplication of the data we need and allow for an experiment without affecting the original data. This will prove useful in the long way.
"""

df = pdDS.copy()
df

"""The DataFrame above showed that we have succesfully loaded and copied the dataset. Based on the short information given, we have around 373 data (rows) complete with about 15 input features (columns).

To have better insight on the input features, below are the detailed descriptions along with their data types:

<center>

|**Column Name**|**Description**|**Data Type**|
|:---|:---|:---|
|**Subject ID**|**Subject Identification (i.e. OAS2_0001)**|**Categorical Nominal**|*|
|**MRI ID**|**MRI Exam Identification (i.e. OAS2_0001_MR1)**|**Categorical Nominal**|
|**Group**|**Class (i.e. Demented, Nondemented)**|**Categorical Nominal**|
|**Visit**|**Number of visits (i.e. 1-5)**|**Categorical Ordinal**|
|**MR Delay**|**Number of days of delay between visits (i.e. 0-2639)**|**Numerical Discrete**|
|**M/F**|**Gender (i.e. M, F)**|**Categorical Nominal**|
|**Hand**|**Right or Left-Handed (i.e. R)**|**Categorical Nominal**|
|**Age**|**Age at time of image acquisition (years). (i.e. 60-98)**|**Numerical Discrete**|
|**EDUC**|**Years of education (i.e. 6-23)**|**Numerical Discrete**|
|**SES**|**Socioeconomic status as assessed by the Hollingshead Index of Social Position and classified into categories from 1 (highest status) to 5 (lowest status) (i.e. 1-5)**|**Numerical Discrete**|
|**MMSE**|**Mini-Mental State Examination (i.e. 4-30)**|**Numerical Discrete**|
|**CDR**|**Clinical Dementia Rating (i.e. 0-2)**|**Numerical Continuous**|
|**eTIV**|**Estimated total intracranial volume (cm^3) (i.e. 1106-2004)**|**Numerical Discrete**|
|**nWBV**|**Normalized whole brain volume: expressed as a percent of all voxels in the atlas-masked image that are labeled as gray or white matter by the automated tissue segmentation process (i.e. 0.64-0.84)**|**Numerical Continuous**|
|**ASF**|**Atlas scaling factor (unitless). Computed scaling factor that transforms native-space brain and skull to the atlas target (i.e. the determinant of the transform matrix) (i.e. 0.88-1.59)**|**Numerical Continuous**|

The table above has provided us with each features data type. However, for better understanding in the programming context, below are the data types generated through the `.info()` function.
"""

df.info()

"""Another information we can look into before proceeding onto the next stage is going through the data using the `.describe()` function. This function specifically offer us a comprehensive report on any feature with number. It shows us the exact count of the data, the lowest and highest value, along with its statistical data such as mean and standard deviation. This information gave us a hint on what to expect next. For example, the range value number varies between features, suggesting that in the later course we may need to implement a scaling transformation procedure to our data."""

df.describe()

"""# **C. Data Preprocessing**

From here we can see that ....

Furthermore, duplicate....
"""

df.duplicated().sum()

df.nunique()

"""Drop uncorrelated: `Subject ID`, `MRI ID`, and `Hand`. both subject ID and MRI ID were dropped due to their inherent value being a unique code for each patients and MRI scans, something which does not have any particular meaning related to Dementia. The Hand variable meanwhile, only have 1 unique value, as seen above, and will not be able to provide any information or variability for the model to learn from."""

df = df.drop(columns=['MRI ID', 'Hand'])

#Check which variable contain missing value ('NaN')
for i, var in enumerate(df):
  if any(df.iloc[:,i].isnull()):
    print(var, df.iloc[:,i].isnull().sum())

"""Based on the previous section, we now know that our dataset contain several missing values. To identify the category of missing values, we can filtered the dataframe to display only the data's with NaN values:"""

df[df.isnull().any(axis=1)]

"""The filtered dataframe provided the necessary insights efficiently. Out of the 12 columns, only two--SES and MMSE--contain  missing values. The SES column has a total of 19 missing entries, while MMSE has 2, representing approximately 5.1% and 0.5% of the total data, respectively. Further inspection revealed that many missing values originate from the same subject IDs, indicating that fewer subjects are responsible for the missing values than initialy appeared.

Closer examination on the variable details uncovered key insight that defined the category of the missing values. The column SES pertains to the subject's economic condition, while the MMSE column reflects the subject's mental condition. Both variables are likely to be left blank for particular reasons such as the subject's reluctance to disclose this information.

These insights combined indicate that the missing values falls under the category of **MNAR** (Missing Not At Random). Normally, missing values in this category require complex approaches to produce optimal inferences. However, to streamline the overall model development, methods such as **multiple imputation** (MI) can be applied using the following procedure to address these missing values.

To impute the missing data, here we will employe the `IterativeImputer` function derived from `Scikit-learn`. This particular imputation method was chosen due to the inherent property of the missing data--Missing Not At Random (MNAR), as discussed previously. This imputation method will ensure sufficient interference computation to produce
"""

#Impute missing data with IterativeImputer from sklearn
df_imputer = IterativeImputer()
imputed_df = df_imputer.fit_transform(df[['SES','MMSE']])

imputed_df = pd.DataFrame(imputed_df, columns=df[['SES','MMSE']].columns)

#Convert continuous data on SES and MMSE into discrete data
imputed_df['SES'] = round(imputed_df['SES'])
imputed_df['MMSE'] = round(imputed_df['MMSE'])

imputed_df.head(5)

df.head()

df['SES'] = imputed_df['SES']
df['MMSE'] = imputed_df['MMSE']
df.head()

"""Before we proceed to the next section, one last thing we must do is convert any value not categorized into 'Dementia' and 'Nondementia'. Because as we learned from the data information above, the 'Group' variable contained as many as 3 unique value. Hence, we need to change them into 'Dementia' first before we can proceed on exploratory data and other steps in this machine learning development.

To do that, first we check the unique value within `Group` variable:
"""

df['Group'].unique()

"""As seen, the third value not fallen into either 'Demented' or 'Nondemented' is categorized as 'Converted'. This value, based on information from data source, refer to patients who was diagnosed as 'Demented' on a later date. In this project, due to inssuficient information, each Converted value will not be separated into 'Demented' and 'Nondemented' on each MRI Scans, and will instead be recategorized as 'Demented'"""

df.loc[df['Group']=='Converted','Group'] = 'Demented'

df['Group'].unique()

"""# **D. Exploratory Data Analysis (EDA)**"""

col_df = df.columns
print(len(df.columns))
col_df

"""### I. Histplot"""

col_hist = ['MR Delay', 'Age', 'MMSE', 'eTIV', 'nWBV', 'ASF']

fig, axes = plt.subplots(3,2, figsize=(13,12))
axes = axes.flatten()

def random_color():
    return [random.random() for _ in range(3)]

for i, var in enumerate(col_hist):
  std = np.std(df[var])
  mn = np.mean(df[var])

  sns.histplot(data=df, x=var, ax=axes[i], kde=True,color=random_color(), edgecolor='black', bins=20)
  axes[i].set_title(f'{var} Distribution Plot')
  axes[i].set_xlabel(var)
  axes[i].set_ylabel('Frequency')
  axes[i].tick_params(labelrotation=35)

  if var == 'MMSE':
    x_y = (0.6,0.85)
  else:
    x_y = (0.7,0.85)

  props = dict(boxstyle='round', edgecolor='black', facecolor='white')
  axes[i].annotate(f'Standar Deviation\n{std:.2f}\nMean: {mn:.2f}', xy=x_y,
                   xycoords="axes fraction", xytext=(8,8), textcoords="offset points",
                   ha='left', va='top', bbox=props)


for j in range(len(col_hist), len(axes)):
    axes[j].axis('off')

plt.tight_layout()
plt.show()

mrd = df[df['MR Delay']>0]

std = np.std(df['MR Delay'])
mn = np.mean(df['MR Delay'])

plt.figure(figsize=(8,5))
sns.histplot(mrd['MR Delay'], label='MR Delay', kde=True,color=random_color(), edgecolor='black', bins=20)
plt.title('MR Delay Distribution Plot')
plt.xlabel('MR Delay')
plt.ylabel('Frequency')
plt.tick_params(labelrotation=35)

props = dict(boxstyle='round', edgecolor='black', facecolor='white')
plt.annotate(f'Standar Deviation\n{std:.2f}\nMean: {mn:.2f}', xy=x_y,
                   xycoords="axes fraction", xytext=(8,8), textcoords="offset points",
                   ha='left', va='top', bbox=props)

plt.tight_layout()
plt.show()

"""### Countplot"""

col_bar = ['Group', 'Visit', 'EDUC','M/F', 'SES', 'CDR']

fig, axes = plt.subplots(3,2, figsize=(10,12))
axes = axes.flatten()

for i, var in enumerate(col_bar):
  sns.countplot(x = var, data=df, ax=axes[i],hue=var, palette='coolwarm')
  axes[i].set_title(f'{var} Count Distribution')
  axes[i].set_xlabel(var)
  axes[i].set_ylabel('Frequency')


for j in range(len(col_bar), len(axes)):
    axes[j].axis('off')

plt.tight_layout()
plt.show()

count = df['MMSE'].value_counts().to_dict()

palette = sns.color_palette('coolwarm')

plt.figure(figsize=(7,7))

plt.pie(count.values(), labels=count.keys(),colors=palette, autopct='%0.1f%%')

plt.tight_layout()
plt.show()

"""# **E. Feature Engineering**"""

df_copy = df.copy().drop(columns='Visit')
df_to_convert = ['Subject ID', 'Group', 'M/F']

for col in df_to_convert:
  df_copy[col] = df_copy[col].astype('category').cat.codes

coded_df = df_copy.groupby(['Subject ID']).mean()
coded_df

visit = df['Visit'].groupby(df_copy['Subject ID']).max()
coded_df.insert(2,'Visit',visit)
coded_df

coded_df.info()

"""###Outlier

To check for outlier within out datasets, we can once again visit the number of unique values within each variables using the `.nunique()` function.
"""

coded_df.nunique()

"""Based on the information above, we can infer that some variables such as Visit, M/F, EDUC, SES, MMSE, and CDR fall into the categorical columns type. Generally, this type of columns can be exempted from outlier detection. Thus, in this section, we will check for outliers from the remaining numerical columns: MR Delay, Age, eTIV, nWBV, and ASF."""

copy_df = coded_df['Group'].copy()
copy_df.head()

copy_df[copy_df==1] = 'Demented'
copy_df[copy_df==0] = 'Nondemented'

copy_df.head(3)

df_out = ['MR Delay', 'Age', 'eTIV', 'nWBV', 'ASF']

def plot_interquartile_box(data,df,y=copy_df):
  fig, axes = plt.subplots(3, 2, figsize=(12,10))
  axes = axes.flatten()

  for i, var in enumerate(df):
    axes[i]
    sns.boxplot(data=data, x=var, y=y, ax=axes[i], medianprops={'color':'r'}, flierprops={'marker':'x'})
    axes[i].set_title(f'Group vs {var}')

  for j in range(len(df_out),len(axes)):
    axes[j].axis('off')

  plt.tight_layout()
  plt.show()

plot_interquartile_box(coded_df,df_out)

"""as we can see, numerical columns such as MR Delay and eTIV shows the presence of outliers. Much more than the other columns. before proceeding to detecting the number of outliers. First, we must check for each column skewness type."""

#Define skewness of each numerical column

def check_skewness(df, cols):
  normal = {}
  skewed = {}
  extremely_skewed = {}

  for var in cols:
    skew_score = round(df[var].skew(),2)
    if skew_score <= 0.5:
      normal[var] = skew_score
    elif 0.5 < abs(skew_score) < 1:
      skewed[var] = skew_score
    else:
      extremely_skewed[var] = skew_score

  return normal, skewed, extremely_skewed

normal, skewed, extremely_skewed = check_skewness(coded_df, df_out)

print(f'Normal: {normal}\nSkewed: {skewed}\nExtremely Skewed: {extremely_skewed}')

#Find Outliers

def check_outlier_percentage(df, skew_type, cols, distance=1.5):
  for var in cols:
    if skew_type == 'normal':
      std = df[var].std()
      mean = df[var].mean()
      lower_bound = mean - (3 * std)
      upper_bound = mean + (3 * std)
    elif skew_type == 'skewed':
      Q1 = df[var].quantile(0.25)
      Q3 = df[var].quantile(0.75)
      IQR = Q3-Q1
      lower_bound = Q1 - (IQR * distance)
      upper_bound = Q3 + (IQR * distance)

    outliers = df[(df[var] < lower_bound) | (df[var] > upper_bound)]
    outlier_percentage = len(outliers) / len(df) * 100
    outlier_count = len(outliers)

    print(f'Outlier Percentage and Count of {var}: {outlier_percentage:.2f}% and {outlier_count}')

check_outlier_percentage(coded_df,'normal',normal.keys())
check_outlier_percentage(coded_df,'skewed',skewed.keys())
check_outlier_percentage(coded_df,'skewed',extremely_skewed.keys())

"""- none of the normal column have any outlier
- MRI Delay have normal outlier count and percentage. Moreover, the nature of
"""

def outlier_data(df, var,distance=1.5):
  Q1 = df[var].quantile(0.25)
  Q3 = df[var].quantile(0.75)
  IQR = Q3-Q1
  lower_bound = Q1 - (IQR * distance)
  upper_bound = Q3 + (IQR * distance)

  print(f'lower bound: {lower_bound:.2f}, upper_bound: {upper_bound:.2f}')

  outlier = df[(df[var] < lower_bound) | (df[var] > upper_bound)]
  return outlier

outlier_data(coded_df,'MR Delay',distance=1.5)

outlier_data(coded_df, 'eTIV',distance=1.5)

"""The presence of outlier, coupled with the nature of the MRI Delay value indicate the need for applying a transformation. Which will be addressed later on.

Outlier Transformation with Winsorizer()
"""

def winsorizer(df, variables, capping_method='iqr', tail='both', fold=1.5):
  winsorizer = Winsorizer(capping_method=capping_method, tail=tail, fold=fold, variables=variables)
  df_trained = winsorizer.fit_transform(df)
  return df_trained

transformed_df = winsorizer(coded_df, ['eTIV','MR Delay'])
transformed_df.head()

"""Re-check outlier"""

plot_interquartile_box(transformed_df,df_out)

check_outlier_percentage(transformed_df,'normal',normal.keys())
check_outlier_percentage(transformed_df,'skewed',skewed.keys())
check_outlier_percentage(transformed_df,'skewed',extremely_skewed.keys())

"""#### Pearson Corelation"""

plt.figure(figsize=(12,6))
corr = transformed_df.corr()
masks = (abs(corr)<=0.2)
sns.heatmap(corr,annot=True,cmap='coolwarm',mask=masks)

corr['Group']

"""- Age, ASF, and eTIV has low correlation with target Group
- MRI Delay and nWBV has higher correlation coeffision however it did not reach past 0.3

which we will

### Imbalance Data Check
"""

val = transformed_df['Group'].value_counts()
val

diff = (val[0] - val[1])/val[0]*100
print(f'Count Percentage Difference between Demented and Nondemented is {round(diff,2)}%')

"""Less than 10%

### Feature Selection

Categorical columns selection

#### Phik Correlation
"""

def phik_correlation(df,cols):
  phik_corr = df[cols].phik_matrix()
  return phik_corr['Group']

categorical_col = ['Group','MMSE', 'Visit', 'EDUC','M/F', 'SES', 'CDR']

phik_corr = phik_correlation(transformed_df, categorical_col)

print(phik_corr[1:])

corr['Group']

"""comparing both sets of correlation matrix, it can be inferred that MMSE, CDR, M/F, EDUC all contain informations that could prove beneficial later on. While the other two columns, SES and Visit, has low correlation in both correlation matrix. Thus it can be dropped."""

phik_selected_df = transformed_df.drop(columns=['Visit','SES'])
phik_selected_df

"""#### Categorical Column value handling"""

phik_selected_df[['M/F','MMSE','EDUC','CDR']].nunique()

def custom_round(val):
  int_part = int(val)
  dec_part = val - int_part
  if 0 < dec_part <= 0.5:
    return int_part + 0.5
  else:
    return int_part + 1.0

phik_selected_df['CDR'] = phik_selected_df['CDR'].apply(custom_round)
phik_selected_df['MMSE'] = np.round(phik_selected_df['MMSE'])
phik_selected_df

phik_selected_df[['M/F','MMSE','EDUC','CDR']].nunique()

"""#### Recursive Feature Elimination (RFE)"""

df_num = ['MR Delay', 'Age', 'eTIV', 'nWBV','ASF']
rfe_x = phik_selected_df[df_num]
rfe_y = phik_selected_df['Group']
rfe_x.head(3)

rfe_mod = SVC(kernel='linear',C=1)

rfe = RFE(rfe_mod,n_features_to_select=3)

rfe.fit(rfe_x,rfe_y)

selected_col = [col for i, col in enumerate(df_num) if rfe.support_[i] == True]
print('Selected features: ', selected_col)

rfe_selected_df = phik_selected_df.drop(columns=['MR Delay', 'eTIV'])
rfe_selected_df.columns

"""# **F. Model Development**

#### Scaling Transformation
"""

numeric = ['Age','nWBV', 'ASF']

scaler = StandardScaler()
transformer = ColumnTransformer(transformers=[('numeric',scaler,numeric)],
                                remainder='passthrough')

X = rfe_selected_df.drop(columns='Group')
y = rfe_selected_df['Group']

transformer.fit(X,y)

"""#### Split Data Train-Test"""

Xtrain, X_test, ytrain, y_test = train_test_split(X,y,train_size=0.95,random_state=42, stratify=y)

Xtrain.shape, X_test.shape, ytrain.shape, y_test.shape

X_train, X_validation, y_train, y_validation = train_test_split(Xtrain,ytrain,train_size=0.90,random_state=42, stratify=ytrain)

X_train.shape, X_validation.shape, y_train.shape, y_validation.shape

"""####Model Pipeline"""

svc_pipe = Pipeline(steps=[('transformer',transformer),('classifier',SVC(kernel='linear',C=1))])
rf_pipe = Pipeline(steps=[('transformer',transformer),('classifier',RandomForestClassifier(max_depth=3,random_state=42))])
dt_pipe = Pipeline(steps=[('transformer',transformer),('classifier',DecisionTreeClassifier(max_depth=3,random_state=42))])
xg_pipe = Pipeline(steps=[('transformer',transformer),('classifier',XGBClassifier(max_depth=3,random_state=42))])
mlp_pipe = Pipeline(steps=[('transformer',transformer),('classifier',MLPClassifier(solver='adam', learning_rate_init=0.01,random_state=42))])

kfold = KFold(n_splits=5, shuffle=True, random_state=42)

svc_cv = cross_val_score(svc_pipe,X_train,y_train,scoring='recall',cv=kfold,n_jobs=-1)
rf_cv = cross_val_score(rf_pipe,X_train,y_train,scoring='recall',cv=kfold,n_jobs=-1)
dt_cv = cross_val_score(dt_pipe,X_train,y_train,scoring='recall',cv=kfold,n_jobs=-1)
xg_cv = cross_val_score(xg_pipe,X_train,y_train,scoring='recall',cv=kfold,n_jobs=-1)
mlp_cv = cross_val_score(mlp_pipe,X_train,y_train,scoring='recall',cv=kfold,n_jobs=-1)

model_name = ""
cv_score = 0

for cv, name in zip([svc_cv,rf_cv,dt_cv,xg_cv,mlp_cv],['SVC','RandomForest','DecisionTree','XGBoost','MLPerceptron']):
  print(name)
  print('Cross-Validation - Result - Recall: ',cv)
  print('Cross-Validation - Mean - Recall:', cv.mean())
  print('Cross-Validation - Std - Recall:', cv.std())
  print('-'*30)

  if cv.mean() > cv_score:
    cv_score = cv.mean()
    model_name = name
  else:
    pass

print('Best CV Score: ', cv_score)
print('Best Model: ', model_name)

"""#### Model Training"""

dt_pipe.fit(X_train,y_train)

y_pred_train = dt_pipe.predict(X_train)
y_pred_validation = dt_pipe.predict(X_validation)

c_matrix_train = confusion_matrix(y_train,y_pred_train,labels=dt_pipe.classes_)
c_visual_train = ConfusionMatrixDisplay(confusion_matrix=c_matrix_train, display_labels=dt_pipe.classes_)

c_matrix_validation = confusion_matrix(y_validation,y_pred_validation,labels=dt_pipe.classes_)
c_visual_validation = ConfusionMatrixDisplay(confusion_matrix=c_matrix_validation, display_labels=dt_pipe.classes_)

fig, axes = plt.subplots(1, 2, figsize=(12,4))

c_visual_train.plot(ax=axes[0],cmap='Blues',)
axes[0].set_title('Train Set - Confusion Matrix')


c_visual_validation.plot(ax=axes[1],cmap='Blues')
axes[1].set_title('Validation Set - Confusion Matrix')

plt.tight_layout()
plt.show()

def report(eval_scores, y_train, y_pred, report_name):
  accuracy = accuracy_score(y_train, y_pred)
  precision = precision_score(y_train, y_pred)
  recall = recall_score(y_train, y_pred)
  F1_score = f1_score(y_train, y_pred)
  auc_score = roc_auc_score(y_train, y_pred)

  scores = {
      'Accuracy' : accuracy,
      'Precision' : precision,
      'Recall' : recall,
      'F1-Score' : F1_score,
      'ROC-AUC' : auc_score
  }

  eval_scores[report_name] = scores

  return eval_scores

eval_scores = {}
eval_scores = report(eval_scores, y_train, y_pred_train, 'Train Set Result')
eval_scores = report(eval_scores, y_validation, y_pred_validation, 'Validation Set Result')

pd.DataFrame(eval_scores)

"""#### Testing on new test data"""

y_pred_test = dt_pipe.predict(X_test)

c_matrix_test = confusion_matrix(y_test,y_pred_test,labels=dt_pipe.classes_)
c_visual_test = ConfusionMatrixDisplay(confusion_matrix=c_matrix_test, display_labels=dt_pipe.classes_)

plt.figure()

c_visual_test.plot(cmap='Blues')
plt.title('Test Set - Confusion Matrix')

plt.tight_layout()
plt.show()

eval_scores = report(eval_scores, y_test, y_pred_test, 'Test Set Result')

pd.DataFrame(eval_scores)

"""# **G. Model Deployment**"""

with open('dt_model.pkl','wb') as file:
  pickle.dump(dt_pipe,file)