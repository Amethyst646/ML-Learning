# -*- coding: utf-8 -*-
"""Demented vs Non Demented Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PS1jAPW5-Yw90rnwjpTpp6-04iToQ1MI

# **Demented vs. Nondemented Classification**
---



**Project Overview**

This project involves developing a Machine Learning model to classify patients as demented or non-demented. It is conducted independently using data from the [MRI and Alzheimer's Kaggle Dataset](https://www.kaggle.com/datasets/jboysen/mri-and-alzheimers?select=oasis_longitudinal.csv).

  **Objectives** :  The primary goal for this project is to understand each step in the machine learning development processes, creating a structured guide for developing and deploying the demented vs non-demented classification model. Key steps covered in this project are as follows:
  - `Data Collection`: Gathering and organizing relevant data for model development and training
  - `Data Preprocessing`: Cleaning and preparation stage before proceeding to the analysis stage.
  - `Exploratory Data Analysis (EDA)`: Analyzing the prepared data through visualization and identifying correlations or patterns within variables.
  - `Feature Engineering`: Feature selection to improve efficiency and model accuracy.
  - `Model Selection`: Testing and choosing the most suitable model for classification.
  - `Hyperparameter Tuning`: Optimizing model parameters to enhance model performance using various techniques.
  - `Model Training`: Training the selected model to learn patterns.
  - `Model Evaluation`: Evaluating and giving assesment on model accuracy and effectiveness using several evaluation metrics.
  - `Model Deployment`: Deploying the model to make prediction in a production setting.

##**A. Data Collection**

To begin, we must first import the necessary libraries. Each library modules will be deployed throughout the model development according to their function and use, as follows:

`Numpy`, `Pandas`, `Pickle`, `Matplotlib`, `Seaborn`, `Random`, `Scikit-learn`
"""

#import libraries
import numpy as np
import pandas as pd
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
import random

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

"""Now that we have declared each of the important libraries, we can finally start loading the dataset. Here, we will transform the dataset file we have from a .csv file into a readable format using **pd.read_csv**:"""

pdDS = pd.read_csv('/content/DementedvsNonDemented.csv')

"""But wait! Before we proceed, there is one more step that we can take. Creating a copy of the table data will prove useful in the long way. We can experiment with the dataframe without affecting the original data."""

DS = pdDS.copy()

"""Once done, now we can take a look at the dataframe we have and gain initial insight on the data types of our variables."""

DS.head(3)

DS.tail(3)

"""Dataset Variable details:

<center>

|**Column Name**|**Description**|**Data Type**|
|:---|:---|:---|
|**Subject ID**|**Subject Identification (i.e. OAS2_0001)**|**Categorical Nominal**|
|**MRI ID**|**MRI Exam Identification (i.e. OAS2_0001_MR1)**|**Categorical Nominal**|
|**Group**|**Class (i.e. Demented, Nondemented)**|**Categorical Nominal**|
|**Visit**|**Number of visits (i.e. 1-5)**|**Ordinal Categorical**|
|**MR Delay**|**Number of days of delay between visits (i.e. 0-2639)**|**Numerical Discrete**|
|**M/F**|**Gender (i.e. M, F)**|**Categorical Nominal**|
|**Hand**|**Right or Left-Handed (i.e. R)**|**Categorical Nominal**|
|**Age**|**Age at time of image acquisition (years). (i.e. 60-98)**|**Numerical Discrete**|
|**EDUC**|**Years of education (i.e. 6-23)**|**Numerical Discrete**|
|**SES**|**Socioeconomic status as assessed by the Hollingshead Index of Social Position and classified into categories from 1 (highest status) to 5 (lowest status) (i.e. 1-5)**|**Numerical Discrete**|
|**MMSE**|**Mini-Mental State Examination (i.e. 4-30)**|**Numerical Discrete**|
|**CDR**|**Clinical Dementia Rating (i.e. 0-2)**|**Numerical Continuous**|
|**eTIV**|**Estimated total intracranial volume (cm^3) (i.e. 1106-2004)**|**Numerical Discrete**|
|**nWBV**|**Normalized whole brain volume: expressed as a percent of all voxels in the atlas-masked image that are labeled as gray or white matter by the automated tissue segmentation process (i.e. 0.64-0.84)**|**Numerical Continuous**|
|**ASF**|**Atlas scaling factor (unitless). Computed scaling factor that transforms native-space brain and skull to the atlas target (i.e. the determinant of the transform matrix) (i.e. 0.88-1.59)**|**Numerical Continuous**|
"""

DS.info()

DS.describe()

#Check which variable contain missing value ('NaN')
for i, var in enumerate(DS):
  if any(DS.iloc[:,i].isnull()):
    print(var, DS.iloc[:,i].isnull().sum())

DS.nunique()

"""From here we can see that ....

Furthermore, duplicate....
"""

DS.duplicated().sum()

"""##**B. Data Preprocessing**

Based on the previous section, we now know that our dataset contain several missing values. To identify the category of missing values, we can filtered the dataframe to display only the data's with NaN values:
"""

DS[DS.isnull().any(axis=1)]

"""The filtered dataframe provided the necessary insights efficiently. Out of the 15 columns, only two--SES and MMSE--contain  missing values. The SES column has a total of 19 missing entries, while MMSE has 2, representing approximately 5.1% and 0.5% of the total data, respectively. Further inspection revealed that many missing values originate from the same subject IDs, indicating that fewer subjects are responsible for the missing values than initialy appeared.

Closer examination on the variable details uncovered key insight that defined the category of the missing values. The column SES pertains to the subject's economic condition, while the MMSE column reflects the subject's mental condition. Both variables are likely to be left blank for particular reasons such as the subject's reluctance to disclose this information.

These insights combined indicate that the missing values falls under the category of MNAR (Missing Not At Random). Normally, missing values in this category require complex approaches to produce optimal inferences. However, to streamline the overall model development, methods such as multiple imputation (MI) can be applied using the following procedure to address these missing values.

To impute the missing data, here we will employe the `IterativeImputer` function derived from `Scikit-learn`. This particular imputation method was chosen due to the inherent property of the missing data--Missing Not At Random (MNAR), as discussed previously. This imputation method will ensure sufficient interference computation to produce
"""

#Impute missing data with IterativeImputer from sklearn
DSimputer = IterativeImputer()
imputedDS = DSimputer.fit_transform(DS[['SES','MMSE']])

imputedDS = pd.DataFrame(imputedDS, columns=DS[['SES','MMSE']].columns)

#Convert continuous data on SES and MMSE into discrete data
imputedDS['SES'] = round(imputedDS['SES'])
imputedDS['MMSE'] = round(imputedDS['MMSE'])

imputedDS.head(5)

DS['SES'] = imputedDS['SES']
DS['MMSE'] = imputedDS['MMSE']
DS.head()

DS['Group'].unique()

DS.loc[DS['Group']=='Converted','Group'] = 'Demented'

DS['Group'].unique()

"""#**C. Exploratory Data Analysis (EDA)**"""

print(len(DS.columns))
DS.columns

"""### I. Histplot"""

fig, axes = plt.subplots(3,2, figsize=(13,12))
col_hist = ['MR Delay', 'Age', 'MMSE', 'eTIV', 'nWBV', 'ASF']
axes = axes.flatten()

def random_color():
    return [random.random() for _ in range(3)]

for i, var in enumerate(col_hist):
  ax = axes[i]
  sns.histplot(data=DS, x=var, ax=ax, kde=True,color=random_color(), edgecolor='black', bins=20)
  ax.set_title(f'{var}')

for j in range(len(col_hist), len(axes)):
    axes[j].axis('off')

plt.tight_layout()
plt.show()

fig, axes = plt.subplots(2,1, figsize=(12,10))
col_name = ['Subject ID','MRI ID']
axes = axes.flatten()

for i, var in enumerate(col_name):
  ax = axes[i]
  sns.histplot(DS.iloc[:,i], label=var,ax=ax, color=random_color(), edgecolor='black', bins=20)
  ax.xaxis.set_visible(False)
  ax.set_title(f'{var}')

mrd = DS[DS['MR Delay']>0]

plt.figure(figsize=(8,5))
sns.histplot(mrd['MR Delay'], label='MR Delay', kde=True,color=random_color(), edgecolor='black', bins=20)

"""### Countplot"""

fig, axes = plt.subplots(4,2, figsize=(10,12))
col_bar = ['Group', 'Visit', 'EDUC','M/F', 'MMSE', 'SES', 'CDR', 'Hand']
axes = axes.flatten()

for i, var in enumerate(col_bar):
  ax = axes[i]
  sns.countplot(x = var, data=DS, ax=ax,hue=var, palette='coolwarm')
  ax.set_title(f'{var}')

for j in range(len(col_bar), len(axes)):
    axes[j].axis('off')

plt.tight_layout()
plt.show()

"""### Corelation plot"""

Coded_DS = DS.copy()
DS_to_convert = ['Subject ID', 'MRI ID', 'Group', 'M/F', 'Hand']

for col in DS_to_convert:
  Coded_DS[col] = Coded_DS[col].astype('category').cat.codes

"""In the context of statistical analysis or model training (which we will explore later), certain datatypes cannot be transformed directly by methods like `IterativeImputer`. As indicated by the previous `.info()` output of our dataframe, several variables in our dataset are categorized as `object`. To resolve this issue, the code above effectively transforms the string values in each `object` column into representative nominal codes using the `cat.codes` function. This conversion facilitates proper handling of categorical data during the imputation process.

The variables was succesfully transformed into nominal datatypes as shown below.
"""

Coded_DS.info()

plt.figure(figsize=(12,6))
corr = Coded_DS.drop(columns=['Hand']).corr()
sns.heatmap(corr,annot=True,cmap='coolwarm')

corr = Coded_DS.corr()
row = corr.index
corr_arr = []
for i,var in enumerate(corr):
  for j,val in enumerate(corr[var]):
    if 0.4<abs(val)<0.99:
      corr_arr.append([var,row[j],round(val,2)])

corr_pd = pd.DataFrame(corr_arr,columns=['First Variable','Second Variable','Value'])

corr_pd

drop_duplicate = []
for i in range(len(corr_pd)):
  for j in range(i+1,len(corr_pd)):
    if (corr_pd.iloc[i,0] == corr_pd.iloc[j,1]) and (corr_pd.iloc[i,1] == corr_pd.iloc[j,0]):
      drop_duplicate.append(j)

corr_pd = corr_pd.drop(drop_duplicate).reset_index(drop=True)

corr_pd

"""Based on the correlatio result above, we can assume that both `MMSE` and `CDR` has the most influence on `Group`. Other variables, on the other hand, doesn't display correlation as high as the two

which we will
"""

